
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Differentiating functionals</title>
    <link rel="stylesheet" href="../../_static/fenics.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '2017.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="../../_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

<link rel="stylesheet" href="../../_static/featured.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
<script src="../../_static/slides.min.jquery.js"></script>
  <script>
	$(function(){
		$('#products').slides({
			preload: true,
			preloadImage: 'img/loading.gif',
			effect: 'slide, fade',
			crossfade: true,
			slideSpeed: 350,
			fadeSpeed: 500,
			generateNextPrev: false,
			generatePagination: false,
	                play: 5000,
                        hoverPause: false,
                        animationStart: function(current){
				$('.caption').animate({
					bottom:-35
				},100);
				if (window.console && console.log) {
					// example return of current slide number
					console.log('animationStart on slide: ', current);
				};
			},
			animationComplete: function(current){
				$('.caption').animate({
					bottom:0
				},200);
				if (window.console && console.log) {
					// example return of current slide number
					console.log('animationComplete on slide: ', current);
				};
			},
			slidesLoaded: function() {
				$('.caption').animate({
					bottom:0
				},200);
			}
		});
	});
  </script>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31806773-1']);
  _gaq.push(['_setDomainName', 'dolfin-adjoint.org']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!-- Load the flex slider library -->
<link rel="stylesheet" href="_static/flexslider/flexslider.css" type="text/css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js"></script>
<script src="_static/flexslider/jquery.flexslider.js"></script>

<script type="text/javascript" charset="utf-8">
  $(window).load(function() {
  $('.flexslider').flexslider();
  });
</script>

<!-- Load the code highlight library -->
<link rel="stylesheet" href="_static/highlight/styles/zenburn.css">
<script src="_static/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<link rel="shortcut icon" href="../../_static/icon.ico" />


  </head>
  <body>
<div class="wrapper">
  <a href="../../"><img src="../../_static/banner.png" width="900px" alt="dolfin-adjoint Project Banner" /></a>
  <div id="access">
    <div class="menu">
      <ul>
          <li class="page_item"><a href="../../about/index.html" title="Find out more about dolfin-adjoint">About</a></li>
          <li class="page_item"><a href="../../features/index.html" title="Features of dolfin-adjoint">Features</a></li>
          <li class="page_item"><a href="../index.html" title="Learn how to use dolfin-adjoint">Documentation</a></li>
          <li class="page_item"><a href="../../download/index.html" title="Obtain the dolfin-adjoint code">Download</a></li>
          <li class="page_item"><a href="../../citing/index.html" title="Learn how to cite the dolfin-adjoint project">Citing</a></li>
          <li class="page_item"><a href="../../support/index.html" title="Where to go for more help">Support</a></li>
      </ul>
    </div><!-- .menu -->
  </div><!-- #access -->
</div><!-- #wrapper -->


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="differentiating-functionals">
<h1>Differentiating functionals<a class="headerlink" href="#differentiating-functionals" title="Permalink to this headline">¶</a></h1>
<div class="section" id="finite-differencing">
<h2>Finite differencing<a class="headerlink" href="#finite-differencing" title="Permalink to this headline">¶</a></h2>
<p>The definition of the derivative
<span class="math">\(\mathrm{d}\widehat{J}/\mathrm{d}m\)</span> is</p>
<div class="math">
\[\frac{\mathrm{d}\widehat{J}}{\mathrm{d}m_i} = \lim_{h \rightarrow 0} \ \frac{\widehat{J}(m + he_i) - \widehat{J}(m)}{h}\]</div>
<p>where <span class="math">\(e_i\)</span> is the vector with 0 in all entries except for 1 in
the <span class="math">\(i^{\mathrm{th}}\)</span> entry.  Each component of the gradient
vector <span class="math">\(\mathrm{d}\widehat{J}/\mathrm{d}m\)</span> is the derivative of
the functional <span class="math">\(\widehat{J}\)</span> with respect to the corresponding
component of <span class="math">\(m\)</span>. A simple idea for approximating the derivative
is to compute each component of the gradient as</p>
<div class="math">
\[\frac{\mathrm{d}\widehat{J}}{\mathrm{d}m_i} \approx \frac{\widehat{J}(m + he_i) - \widehat{J}(m)}{h}\]</div>
<p>for some small choice of <span class="math">\(h\)</span>. The advantage of this approach is
that it is very straightforward: it still only requires a black-box
evaluator for <span class="math">\(\widehat{J}\)</span>, and the approximation of the
gradients can be done entirely within the optimisation algorithm.</p>
<div class="sidebar">
<p class="first sidebar-title">When finite differencing is useful</p>
<p class="last">In the PDE-constrained optimisation case, finite differencing isn’t
very useful for computing the gradient of the functional. However,
it is <em>very</em> useful for rigorously <em>verifying</em> gradients computed
with another approach. For more details, see the section of the
dolfin-adjoint documentation on <a class="reference internal" href="../verification.html"><span class="doc">verifying functional gradients</span></a>.</p>
</div>
<p>However, this approach suffers from several serious drawbacks. One
problem is that it is not obvious how to choose an appropriate value
for <span class="math">\(h\)</span>: choose <span class="math">\(h\)</span> too large, and the finite difference
will not approximate the limit value; choose <span class="math">\(h\)</span> too small, and
numerical precision will destroy the accuracy of the approximation. A
more serious problem, however, is that this approximation requires one
functional evaluation for each degree of freedom in the parameter
space.  When each functional evaluation requires an expensive PDE
solve, this approach quickly becomes impractical, and a more
intelligent algorithm is required.</p>
</div>
<div class="section" id="the-tangent-linear-approach">
<h2>The tangent linear approach<a class="headerlink" href="#the-tangent-linear-approach" title="Permalink to this headline">¶</a></h2>
<p>Recall that <span class="math">\(\widehat{J}(m)\)</span> is the functional considered as a
pure function of <span class="math">\(m\)</span>:</p>
<div class="math">
\[\widehat{J}(m) = J(u(m), m).\]</div>
<p>Let us apply the chain rule to <span class="math">\(\widehat{J}(m)\)</span>:</p>
<div class="math">
\[\underset{\scriptsize{1 \times M}}{\frac{\mathrm{d}\widehat{J}}{\mathrm{d}m}} =
\underset{\scriptsize{1 \times U}}{\frac{\partial J}{\partial u}}
\underset{\scriptsize{U \times M}}{\frac{\mathrm{d}u}{\mathrm{d}m}} +
\underset{\scriptsize{1 \times M}}{\frac{\partial J}{\partial m}}.\]</div>
<p>Let us inspect each term of this relationship, and build up some
intuition about each.  <span class="math">\({\partial J}/{\partial m}\)</span> and
<span class="math">\({\partial J}/{\partial u}\)</span> are typically very straightforward
to compute: <span class="math">\(J\)</span> is usually a simple closed-form expression in
terms of <span class="math">\(u\)</span> and <span class="math">\(m\)</span>, and so their differentiation by hand
is generally trivial. Both of these quantities are vectors, with
dimensions of the parameter space and solution space respectively. By
contrast, the solution Jacobian <span class="math">\({\mathrm{d}u}/{\mathrm{d}m}\)</span> is
rather difficult to compute.  This object is a massive dense matrix,
of dimensions (solution space <span class="math">\(\times\)</span> parameter space), and as
such it is unlikely to fit in memory. However, let us temporarily
suppose that the number of parameters is small, and that we would like
to compute <span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span> using the
relationship above.</p>
<p>With the PDE <span class="math">\(F(u, m) = 0\)</span>, we have an relationship for
<span class="math">\(u\)</span> as an implicit function of <span class="math">\(m\)</span>.  If we take the total
derivative of this equation with respect to <span class="math">\(m\)</span>, we will have a
relationship for the solution Jacobian
<span class="math">\({\mathrm{d}u}/{\mathrm{d}m}\)</span>:</p>
<div class="math">
\[\begin{split}&amp; \frac{\mathrm{d}}{\mathrm{d}m} F(u, m) = \frac{\mathrm{d}}{\mathrm{d}m} 0 \\
\implies &amp; \frac{\partial F(u, m)}{\partial u} \frac{\mathrm{d}u}{\mathrm{d}m} + \frac{\partial F(u, m)}{\partial m} = 0 \\
\implies &amp;
\underset{\scriptsize{U \times U}}{\frac{\partial F(u, m)}{\partial u}}
\underset{\scriptsize{U \times M}}{\frac{\mathrm{d}u}{\mathrm{d}m}} =
\underset{\scriptsize{U \times M}}{-\frac{\partial F(u, m)}{\partial m}}.\end{split}\]</div>
<div class="sidebar">
<p class="first sidebar-title">The tangent linear system</p>
<p class="last">The tangent linear system is the same idea as the <em>forward mode</em> of
algorithmic or automatic differentiation.</p>
</div>
<p>This last relationship is <strong>the tangent linear equation</strong> (or tangent
linear system) associated with the PDE <span class="math">\(F(u, m) = 0\)</span>. Let us
carefully consider each term in the tangent linear system, and build
up some intuition about each.</p>
<p><span class="math">\({\mathrm{d}u}/{\mathrm{d}m}\)</span> is the solution Jacobian again,
with which we can compute the functional gradient
<span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span>. It is the prognostic
variable of this equation, the unknown quantity in the tangent linear
system.</p>
<p>Now consider <span class="math">\({\partial F(u, m)}/{\partial u}\)</span>. Since <span class="math">\(F\)</span>
is a vector expression, its derivative with respect to <span class="math">\(u\)</span>
<span class="math">\(m\)</span> is an operator (a matrix); this operator acts on the
solution Jacobian, and therefore must be inverted or
solved. <span class="math">\(F(u, m)\)</span> may have been nonlinear in <span class="math">\(u\)</span>, but
<span class="math">\({\partial F(u, m)}/{\partial u}\)</span> is always linear. In other
words, <span class="math">\({\partial F(u, m)}/{\partial u}\)</span> <em>is the linearisation
of the equation operator, linearised about a particular solution</em>
<span class="math">\(u\)</span>. If <span class="math">\(F(u, m)\)</span> happened to be linear in the first
place, and so <span class="math">\(F(u, m) \equiv A(m)u - b(m)\)</span> for some operator
<span class="math">\(A(m)\)</span>, then <span class="math">\({\partial F(u, m)}/{\partial u}\)</span> is just the
operator <span class="math">\(A(m)\)</span> back again.</p>
<p>Finally, consider the term <span class="math">\({\partial F(u, m)}/{\partial
m}\)</span>. Like <span class="math">\({\mathrm{d}u}/{\mathrm{d}m}\)</span>, this is a matrix of
dimension (solution space <span class="math">\(\times\)</span> parameter space). This term
acts as the source term for the tangent linear system; each column of
<span class="math">\({\partial F(u, m)}/{\partial m}\)</span> provides the source term for
the derivative of <span class="math">\(u\)</span> with respect to one scalar entry in the
parameter vector.</p>
<p>So, <em>when is solving the tangent linear system a sensible approach</em>?
To answer this question, notice that we had to specify some parameter
<span class="math">\(m\)</span> to construct the tangent linear system, but that the
functional <span class="math">\(J\)</span> does not appear at all.  In other words, <strong>for a
given parameter (input), the tangent linear solution can be used to
easily compute the gradient of any functional</strong>. This means that
solving the tangent linear system makes sense <em>when there are a small
number of parameters (inputs), and a large number of functionals of
interest (outputs)</em>. However, this is generally not the case in
PDE-constrained optimisation. Is there a better way?</p>
</div>
<div class="section" id="the-adjoint-approach">
<h2>The adjoint approach<a class="headerlink" href="#the-adjoint-approach" title="Permalink to this headline">¶</a></h2>
<p>Let us rephrase the tangent linear approach to computing the
gradient. We start by fixing our choice of parameter <span class="math">\(m\)</span>, and
then solve for the solution Jacobian
<span class="math">\({\mathrm{d}u}/{\mathrm{d}m}\)</span> associated with that choice of
<span class="math">\(m\)</span>. With this quantity in hand, we take its inner product with
a source term <span class="math">\({\partial J}/{\partial u}\)</span> particular to the
functional <span class="math">\(J\)</span>, and can then compute the gradient
<span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span>.</p>
<p>Notice that we first fixed the parameter <span class="math">\(m\)</span>, (the “denominator”
of the gradient <span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span>) and
<em>then</em> chose which functional we wished to compute the gradient of
(the “numerator” of the gradient). Is there a way where we could do
the opposite: first fix the functional <span class="math">\(J\)</span>, and <em>then</em> choose
which parameter to take the gradient with respect to? The answer is
yes, and that approach is referred to as the adjoint approach.</p>
<p>Suppose the tangent linear system is invertible. Then we can rewrite
the solution Jacobian as</p>
<div class="math">
\[\frac{\mathrm{d}u}{\mathrm{d}m} = - \left(\frac{\partial F(u, m)}{\partial u}\right)^{-1}
\frac{\partial F(u, m)}{\partial m}.\]</div>
<p>We usually could not compute this expression (computing the inverse of
the operator <span class="math">\({\partial F(u, m)}/{\partial u}\)</span> is prohibitive),
but we can still use it and reason about it. Let us substitute this
expression for the solution Jacobian into the expression for the
gradient of <span class="math">\(\widehat{J}\)</span>:</p>
<div class="math">
\[\begin{split}&amp; \frac{\mathrm{d}\widehat{J}}{\mathrm{d}m} = \frac{\partial J}{\partial u} \frac{\mathrm{d}u}{\mathrm{d}m} + \frac{\partial J}{\partial m}.\\
\implies &amp; \frac{\mathrm{d}\widehat{J}}{\mathrm{d}m} = - \frac{\partial J}{\partial u} \left(\frac{\partial F(u, m)}{\partial u}\right)^{-1} \frac{\partial F(u, m)}{\partial m} + \frac{\partial J}{\partial m}.\end{split}\]</div>
<div class="sidebar">
<p class="first sidebar-title">The adjoint of a matrix</p>
<p class="last">The notation <span class="math">\(A^*\)</span> means to take the transpose of <span class="math">\(A\)</span>,
<span class="math">\(A^T\)</span>, and take the complex conjugate of each entry. If the
matrix <span class="math">\(A\)</span> is composed entirely of real numbers, then the
adjoint is just the transpose. Other words for the adjoint are the
Hermitian and the conjugate transpose.</p>
</div>
<p>Now let’s take the adjoint (Hermitian transpose) of the above equation:</p>
<div class="math">
\[\underset{\scriptsize{M \times 1}}{\frac{\mathrm{d}\widehat{J}}{\mathrm{d}m}^*} =
  -\underset{\scriptsize{M \times U}}{\frac{\partial F}{\partial m}^*}
  \underset{\scriptsize{U \times U}}{\frac{\partial F}{\partial u}^{-*}}
  \underset{\scriptsize{U \times 1}}{\frac{\partial J}{\partial u}^{*}}
  +
  \underset{\scriptsize{M \times 1}}{\frac{\partial J}{\partial m}^*}\]</div>
<p>Let us gather the solution of the inverse Jacobian acting on a vector, and define it
to be a new variable:</p>
<div class="math">
\[\begin{split}&amp; \lambda = \left(\frac{\partial F(u, m)}{\partial u}\right)^{-*} \frac{\partial J}{\partial u}^* \\
\implies &amp; \left(\frac{\partial F(u, m)}{\partial u}\right)^{*} \lambda = \frac{\partial J}{\partial u}^*.\end{split}\]</div>
<div class="sidebar">
<p class="first sidebar-title">The adjoint system</p>
<p>The adjoint system is the same idea as the <em>reverse mode</em> of
algorithmic or automatic differentiation.</p>
<p class="last">Another word for “adjoint” used in the literature is “dual”: people
refer to the dual system, the dual solution, etc.</p>
</div>
<p>This relationship is the <strong>adjoint equation</strong> (or adjoint system)
associated with the PDE <span class="math">\(F(u, m) = 0\)</span>. Again, let us carefully
consider each term in the adjoint equation and build up some intuition
about each.</p>
<p><span class="math">\(\lambda\)</span> is the <em>adjoint variable associated with</em>
<span class="math">\(u\)</span>. Each component of the solution <span class="math">\(u\)</span> will have a
corresponding adjoint variable. For example, if <span class="math">\(F\)</span> is the
Navier-Stokes equations, and <span class="math">\(u\)</span> is the tuple of velocity and
pressure, then <span class="math">\(\lambda\)</span> is the tuple of adjoint velocity and
adjoint pressure. Similarly, if the problem is time-dependent, the
adjoint is also time-dependent, with each variable through time having
a corresponding adjoint value.</p>
<p><span class="math">\(\left({\partial F(u, m)}/{\partial u}\right)^{*}\)</span> is the
<em>adjoint of the tangent linear operator</em>. Commonly, this is referred
as the “adjoint operator”. By taking the transpose, we <em>reverse the
flow of information in the equation system</em>. For example, if a tracer
is advected downstream (and so information about upstream conditions
is advected with it), the adjoint PDE advects information in the
reverse sense, i.e. upstream. This extends to the temporal propagation
of information: if <span class="math">\(F(u, m)\)</span> is a time-dependent PDE (and so
propagates information from earlier times to later times), the adjoint
PDE <em>runs backwards in time</em> (propagates information from later times
to earlier times). This property will be examined in more detail in
the next section.</p>
<p><span class="math">\({\partial J}/{\partial u}\)</span> is the source term for the adjoint
equation. It is this source term that makes an adjoint solution
<em>specific to a particular functional</em>. Just as one cannot speak of the
tangent linear solution without referring to a particular choice of
parameter, one cannot speak of the adjoint solution without referring
to a specific choice of functional.</p>
<p>As the tangent linear operator is always linear, the adjoint is linear
in <span class="math">\(u\)</span> also, and so the adjoint equation is always linear in
<span class="math">\(\lambda\)</span>. This property will also be examined in more detail in
the next section.</p>
<p>So, to compute the functional gradient
<span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span>, we first solve the
adjoint equation for <span class="math">\(\lambda\)</span> (fixing the “nominator” of the
gradient, as the adjoint is specific to the functional), and then take
its inner product with respect to <span class="math">\(-{\partial F(u, m)}/{\partial
m}\)</span> to compute the gradient with respect to a particular parameter
<span class="math">\(m\)</span> (fixing the “denominator” of the gradient). This is
precisely the <em>dual</em> approach to that of computing
<span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span> using the tangent linear
approach, and has precisely the dual scaling: <strong>for a given functional
(output), the adjoint solution can be used to easily compute the
gradient with respect to any parameter</strong>. Therefore, solving the
adjoint system is extremely efficient <em>when there are a small number
of functionals (outputs), and a large number of parameters
(inputs)</em>. This is precisely the case we are considering in
PDE-constrained optimisation: there is one functional (output) of
interest, but many parameters.</p>
<p>So, with some knowledge of the chain rule and some transposition,
we have devised an algorithm for computing the gradient
<span class="math">\({\mathrm{d}\widehat{J}}/{\mathrm{d}m}\)</span> that is extremely
efficient for our case where we have many parameters and only one
functional.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>A sketch of the solution approach for the PDE-constrained optimisation
problem is therefore:</p>
<ol class="arabic simple">
<li>Start with some initial guess for the parameters <span class="math">\(m\)</span>.</li>
<li>Compute the functional <span class="math">\(\widehat{J}(m)\)</span> (using the forward model) and its gradient (using the adjoint model).</li>
<li>Pass these values to an optimisation algorithm.
This algorithm returns a new point in parameter space with a better functional value.</li>
<li>If the gradient is zero, or if the maximum number of iterations has been reached, terminate. Otherwise, go to step 2.</li>
</ol>
<p>Of course, PDE-constrained optimisation is a much richer field than
the simple sketch above would suggest.  Much work is focussed on
exploiting particular properties of the equations or the functional,
ensuring the gradient is represented with the correct Riesz representer,
or imposing additional constraints on the parameter space, or
exploiting advanced forward modelling concepts such as error
estimation, goal-based adaptivity and reduced-order
modelling. Nevertheless, although complications proliferate, the above
algorithm captures the key idea of many approaches used for solving
problems of enormous importance.</p>
<p>With the adjoint and tangent linear equations now introduced, let us
examine them more thoroughly, in <a class="reference internal" href="4-adjoint.html"><span class="doc">the next section</span></a>.</p>
<p class="rubric">References</p>
<p id="bibtex-bibliography-documentation/maths/3-gradients-0"></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2017, Sebastian Mitusch.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.5.
    </div>
  </body>
</html>